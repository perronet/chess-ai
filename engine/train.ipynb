{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import setup\n",
    "import parse\n",
    "import utils\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.activations import relu\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.callbacks import TerminateOnNaN, EarlyStopping\n",
    "from sklearn.model_selection import train_test_split, KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(setup.DATASET_DIR+setup.DATASET, nrows=setup.N_ROWS, dtype={\"Fen\": np.string_, \"Evaluation\": np.string_})\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize the dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each board is converted from FEN to the following vectorized representation:\n",
    "\n",
    "TODO\n",
    "\n",
    "We discard the move counter and half-move clock provided in the last field in the FEN format. This means that we will have to take care of draws manually.\n",
    "\n",
    "Un-normalized scores can be as high/low as 15k. Each score is converted to a range of -1 - 1 using a sigmoid to reflect the likelyhood of winning.\n",
    "\n",
    "| **Output type**                 | **Value range** |\n",
    "|----------------|--------------------------|\n",
    "| Score             | 0 - 1                    |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [f\"f_{str(x)}\" for x in range(1, setup.N_FEATURES+1)]\n",
    "df_vectorized = pd.DataFrame(df[\"FEN\"].apply(lambda fen_str: parse.fen_to_vector(fen_str)).to_list(), columns=features)\n",
    "df_vectorized[\"label\"] = df.apply(lambda row: parse.convert_stockfish_eval(row.FEN, row.Evaluation), axis=1)\n",
    "df_vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for null values\n",
    "assert df_vectorized[df_vectorized.isnull().values].empty\n",
    "# Check shape (+1 is for the label)\n",
    "assert df_vectorized.shape == (setup.N_ROWS, setup.N_FEATURES + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split in training set and cross-validation set (shuffles randomly and splits)\n",
    "X_train, X_cv, y_train, y_cv = train_test_split(df_vectorized[features], df_vectorized[\"label\"], train_size=setup.TRAINING_SET_SIZE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalizer = tf.keras.layers.Normalization(axis=-1)\n",
    "# normalizer.adapt(X)\n",
    "'''\n",
    "TODOs\n",
    "* Try normalizing and check if we are doing it properly\n",
    "* Try elu and leakyrelu activations\n",
    "* Try SGD\n",
    "'''\n",
    "def create_model():\n",
    "    model = Sequential(\n",
    "        [\n",
    "            # normalizer,\n",
    "            Input(shape=(setup.N_FEATURES,)),\n",
    "            Dense(1024, activation=setup.HIDDEN_ACTIVATION, kernel_regularizer=tf.keras.regularizers.l2(setup.REGULARIZATION_RATE)),\n",
    "            Dense(1024, activation=setup.HIDDEN_ACTIVATION, kernel_regularizer=tf.keras.regularizers.l2(setup.REGULARIZATION_RATE)),\n",
    "            Dense(1024, activation=setup.HIDDEN_ACTIVATION, kernel_regularizer=tf.keras.regularizers.l2(setup.REGULARIZATION_RATE)),\n",
    "            Dense(1, activation=setup.OUTPUT_ACTIVATION),\n",
    "        ]\n",
    "    )\n",
    "    model.compile(\n",
    "        loss=MeanSquaredError(),\n",
    "        optimizer=Adam(learning_rate=setup.LEARNING_RATE),\n",
    "        # optimizer=SGD(learning_rate=setup.LEARNING_RATE, nesterov=True, momentum=0.7),\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "model = create_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_evaluate_model(model, X_train, y_train, X_cv, y_cv):\n",
    "    active_callbacks = [TerminateOnNaN()]\n",
    "    if setup.EARLY_STOPPING:\n",
    "        # Early stopping on training set loss\n",
    "        active_callbacks.append(EarlyStopping(monitor=\"loss\", patience=setup.PATIENCE))\n",
    "        # Early stopping on cross-valiation set loss\n",
    "        active_callbacks.append(EarlyStopping(monitor=\"val_loss\", patience=setup.PATIENCE))\n",
    "\n",
    "    history = model.fit(\n",
    "        X_train, y_train, validation_data=(X_cv, y_cv),\n",
    "        epochs=setup.EPOCHS, batch_size=setup.BATCH_SIZE, \n",
    "        callbacks=active_callbacks\n",
    "    )\n",
    "\n",
    "    train_error = model.evaluate(X_train, y_train)\n",
    "    cv_error = model.evaluate(X_cv, y_cv)\n",
    "    return history, train_error, cv_error\n",
    "\n",
    "history, train_error, cv_error = train_evaluate_model(model, X_train, y_train, X_cv, y_cv)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate error and plot learning curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.plot_learning_curve(history)\n",
    "print(f\"Training set error: {train_error:.2}\")\n",
    "print(f\"Cross-validation set error: {cv_error:.2}\")\n",
    "print(f\"Random classifier error: {((np.random.rand(len(y_cv)) - y_cv)**2).mean():.2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if setup.SAVE_MODEL:\n",
    "    model.save(setup.MODEL_NAME)\n",
    "# model = tensorflow.keras.models.load_model(setup.MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if setup.K_FOLD:\n",
    "    k_fold = KFold(n_splits=setup.N_FOLDS)\n",
    "    train_error_all = []\n",
    "    cv_error_all = []\n",
    "    X = df_vectorized[features]\n",
    "    y = df_vectorized[\"label\"]\n",
    "\n",
    "    # TODO reset index first?\n",
    "    for i, (train, test) in enumerate(k_fold.split(X, y)):\n",
    "        model = create_model()\n",
    "        _, train_error, cv_error = train_evaluate_model(model, X.loc[train], y.loc[train], X.loc[test], y.loc[test])\n",
    "        train_error_all.append(train_error)\n",
    "        cv_error_all.append(cv_error)\n",
    "        print(f\"Model #{i+1} done! CV error: {cv_error:.2}\")\n",
    "        del model\n",
    "\n",
    "    print(f\"Training sets error:{train_error_all}\")\n",
    "    print(f\"Cross-validation sets sets errors:{cv_error_all}\")\n",
    "    print(f\"Training sets mean error: {np.mean(np.array(train_error_all)):.2}\")\n",
    "    print(f\"Cross-validation sets mean error: {np.mean(np.array(cv_error_all)):.2}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
